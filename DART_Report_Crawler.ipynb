{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DART(전자공시)에 게시된 KOSPI 상장사 사업보고서 내용, 전처리 없이 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연간 사업보고서만 크롤링 (분기보고서 까지 크롤링 하는 코드는 별도로..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from dateutil.parser import parse as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#콘솔창 넓게 보기\n",
    "pd.set_option('display.width', 400)\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOSPI 10-K(사업보고서) 1~10번 항목 전체 내용 크롤러\n",
    "def get_yearly_rcpt(company_code, date):\n",
    "    API_Key = '삭제'\n",
    "\n",
    "    if len(str(company_code)) == 7: company_code = company_code[1:]\n",
    "    print('Company Code :', company_code)\n",
    "    if type(date) == str: date = p(date)\n",
    "    print('Today :', date)\n",
    "\n",
    "    url = \"http://opendart.fss.or.kr/api/list.xml?crtfc_key=\" + API_Key + \"&corp_code=\" + company_code + \"&bgn_de=19900101&pblntf_detail_ty=A001&corp_cls=Y&page_count=60\" + \"&last_reprt_at=Y\"\n",
    "\n",
    "    xmlsoup = BeautifulSoup(urlopen(url).read(), 'html.parser')\n",
    "\n",
    "    te = xmlsoup.findAll(\"list\")\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for t in te:\n",
    "        time.sleep(0.5)\n",
    "        if '사업보고서' in str(te[0]) :\n",
    "            temp = pd.DataFrame(([\n",
    "                [t.corp_cls.string, t.corp_name.string, t.stock_code.string, t.report_nm.string, t.rcept_no.string,\n",
    "                 t.flr_nm.string, t.rcept_dt.string, t.rm.string]]),\n",
    "                                columns=[\"corp_cls\", \"corp_nm\", \"corp_code\", \"report_nm\", \"rcept_no\", \"flr_nm\", \"rcept_dt\", \"rmk\"])\n",
    "            data = pd.concat([data, temp])\n",
    "\n",
    "    if len(data) != 0 :\n",
    "        data = data.reset_index(drop=True)\n",
    "        data['y_sort'] = data['report_nm'].apply(lambda x: str(x[-8:]).split('.')[0])\n",
    "        data = data.sort_values('y_sort', ascending=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첨부정정 리포트까지 접근 & 저장 (! 첨부정정 사항이 있는 경우, 필요한 사업보고서에 바로 접근이 안됨)\n",
    "def Year_Report_Crawler_v2(company_code, today):\n",
    "    \n",
    "    data = get_yearly_rcpt(company_code, today)\n",
    "\n",
    "    if len(data) != 0 :\n",
    "        urls = []\n",
    "        pages = []\n",
    "\n",
    "        for i in range(len(data['rcept_no'])):\n",
    "            if '[첨부정정]' in str(data['report_nm'][i]):\n",
    "                data['rcept_no'][i] = str(BeautifulSoup(urlopen(\"http://dart.fss.or.kr/dsaf001/main.do?rcpNo=\" +\n",
    "                                                                     data['rcept_no'][i]).read(), 'html.parser').find('select').find_all('option')[1]).split(\"value=\")[1].split(\">\")[0].split(\"rcpNo=\")[1].split('\"')[0]\n",
    "\n",
    "        for i in range(len(data['rcept_no'])):\n",
    "            urls.append(\n",
    "                \"http://dart.fss.or.kr/dsaf001/main.do?rcpNo=\" + data['rcept_no'][i])  # 보고서 발행일자별로 link 생성해줌\n",
    "\n",
    "        for i in range(len(urls)):\n",
    "            pages.append(BeautifulSoup(urlopen(urls[i]).read(), 'html.parser'))\n",
    "\n",
    "        body1 = []\n",
    "        body2 = []\n",
    "        body3 = []\n",
    "        body4 = []\n",
    "        body5 = []\n",
    "        body6 = []\n",
    "        body7 = []\n",
    "        body8 = []\n",
    "        body9 = []\n",
    "        body10 = []\n",
    "\n",
    "        # 기업별로 전체 기간에 대해 사업보고서 1~10번 항목의 내용을 전부 수집\n",
    "        for i in tqdm(range(len(pages))):\n",
    "\n",
    "            time.sleep(1)\n",
    "            if 'I. 회사의 개요\"' in str(pages[i].find('head')):\n",
    "                body1.append(\n",
    "                    str(pages[i].find('head')).split('I. 회사의 개요\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            if 'I. 회사의 개황\"' in str(pages[i].find('head')):\n",
    "                body1.append(\n",
    "                    str(pages[i].find('head')).split('I. 회사의 개황\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[\n",
    "                        0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "            if ('II. 사업의 내용(\"' not in str(pages[i].find('head'))) and ('II. 사업의 내용\"' in str(pages[i].find('head'))):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            if 'II. 사업의 내용(제조업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(제조업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'II. 사업의 내용(은행업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(은행업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            if 'II. 사업의 내용(보험업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(보험업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "\n",
    "            if 'II. 사업의 내용(증권업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(증권업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "\n",
    "            if 'II. 사업의 내용(도소매업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(도소매업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "\n",
    "            if 'II. 사업의 내용(건설업)\"' in str(pages[i].find('head')):\n",
    "                body2.append(\n",
    "                    str(pages[i].find('head')).split('II. 사업의 내용(건설업)\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            if 'III. 재무에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body3.append(\n",
    "                    str(pages[i].find('head')).split('III. 재무에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            if 'IV. 감사인의 감사의견 등\"' in str(pages[i].find('head')):\n",
    "                body4.append(\n",
    "                    str(pages[i].find('head')).split('IV. 감사인의 감사의견 등\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "\n",
    "            if 'IV. 감사인의 감사의견 등\"' not in str(pages[i].find('head')) and 'V. 감사인의 감사의견 등\"' in str(pages[i].find('head')):\n",
    "                body4.append(\n",
    "                    str(pages[i].find('head')).split('V. 감사인의 감사의견 등\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "            if 'VII. 주주에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body5.append(\n",
    "                    str(pages[i].find('head')).split('VII. 주주에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'VII. 주주에 관한 사항\"' not in str(pages[i].find('head')) and 'VI. 주식에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body5.append(\n",
    "                    str(pages[i].find('head')).split('VI. 주식에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if ('VII. 주주에 관한 사항\"' not in str(pages[i].find('head')) and 'VI. 주식에 관한 사항\"' not in str(pages[i].find('head'))) and 'VI. 주주에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body5.append(\n",
    "                    str(pages[i].find('head')).split('VI. 주주에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if ('VII. 주주에 관한 사항\"' not in str(pages[i].find('head')) and 'VI. 주식에 관한 사항\"' not in str(pages[i].find('head')) and 'VI. 주주에 관한 사항\"' not in str(pages[i].find('head'))) and 'V. 주주에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body5.append(\n",
    "                    str(pages[i].find('head')).split('V. 주주에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            time.sleep(1)\n",
    "            if 'VIII. 임원 및 직원 등에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body6.append(\n",
    "                    str(pages[i].find('head')).split('VIII. 임원 및 직원 등에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'VIII. 임원 및 직원 등에 관한 사항\"' not in str(pages[i].find('head')) and 'VII. 임원 및 직원 등에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body6.append(\n",
    "                    str(pages[i].find('head')).split('VII. 임원 및 직원 등에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "            if 'X. 이해관계자와의 거래내용\"' in str(pages[i].find('head')):\n",
    "                body7.append(\n",
    "                    str(pages[i].find('head')).split('X. 이해관계자와의 거래내용\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'X. 이해관계자와의 거래내용\"' not in str(pages[i].find('head')) and 'IX. 이해관계자와의 거래내용\"' in str(pages[i].find('head')):\n",
    "                body7.append(\n",
    "                    str(pages[i].find('head')).split('IX. 이해관계자와의 거래내용\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(\n",
    "                        ')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if ('X. 이해관계자와의 거래내용\"' not in str(pages[i].find('head'))) and ('IX. 이해관계자와의 거래내용\"' not in str(pages[i].find('head'))) and 'VIII. 이해관계자와의 거래내용\"' in str(pages[i].find('head')):\n",
    "                body7.append(\n",
    "                    str(pages[i].find('head')).split('VIII. 이해관계자와의 거래내용\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(\n",
    "                        ')')[0].split(', ')\n",
    "                )\n",
    "            time.sleep(1)\n",
    "\n",
    "            if 'VI. 이사회 등 회사의 기관에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('VI. 이사회 등 회사의 기관에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'VI. 이사회 등 회사의 기관에 관한 사항\"' not in str(pages[i].find('head')) and 'VI. 이사회 등 회사의 기관 및 계열회사에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('VI. 이사회 등 회사의 기관 및 계열회사에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'V. 이사회 등 회사의 기관 및 계열회사에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('V. 이사회 등 회사의 기관 및 계열회사에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            time.sleep(1)\n",
    "            if 'IX. 계열회사 등에 관한 사항\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('IX. 계열회사 등에 관한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[\n",
    "                        1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'V. 지배구조 및 관계회사 등의 현황\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('V. 지배구조 및 관계회사 등의 현황\",')[1].split('cnt++')[0].split('viewDoc(')[\n",
    "                        1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'V. 지배구조 및 관계회사 등의 현황\"' not in str(pages[i].find('head')) and 'V. 지배구조 및 관계회사등의 현황\"' in str(pages[i].find('head')):\n",
    "                body8.append(\n",
    "                    str(pages[i].find('head')).split('V. 지배구조 및 관계회사등의 현황\",')[1].split('cnt++')[0].split('viewDoc(')[\n",
    "                        1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "            if 'XI. 그 밖에 투자자 보호를 위하여 필요한 사항\"' in str(pages[i].find('head')):\n",
    "                body9.append(\n",
    "                    str(pages[i].find('head')).split('XI. 그 밖에 투자자 보호를 위하여 필요한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'XI. 그 밖에 투자자 보호를 위하여 필요한 사항\"' not in str(pages[i].find('head')) and 'X. 그 밖에 투자자 보호를 위하여 필요한 사항\"' in str(pages[i].find('head')):\n",
    "                body9.append(\n",
    "                    str(pages[i].find('head')).split('X. 그 밖에 투자자 보호를 위하여 필요한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(\n",
    "                        ')')[0].split(', ')\n",
    "                )\n",
    "            #time.sleep(1)\n",
    "            if 'X. 기타 필요한 사항\"' in str(pages[i].find('head')):\n",
    "                body9.append(\n",
    "                    str(pages[i].find('head')).split('X. 기타 필요한 사항\",')[1].split('cnt++')[0].split('viewDoc(')[\n",
    "                        1].split(')')[0].split(', ')\n",
    "                )\n",
    "\n",
    "            # if 'XII. 부속명세서\"' in str(pages1[i].find('head')) :\n",
    "            #     body9.append(\n",
    "            #         str(pages1[i].find('head')).split('XII. 부속명세서\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(\n",
    "            #             ')')[0].split(', ')\n",
    "            #     )\n",
    "            #\n",
    "            # if 'IX. 부속명세서\"' in str(pages1[i].find('head')) :\n",
    "            #     body9.append(\n",
    "            #         str(pages1[i].find('head')).split('IX. 부속명세서\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(\n",
    "            #             ')')[0].split(', ')\n",
    "            #     )\n",
    "            \n",
    "            time.sleep(1)\n",
    "            if 'IV. 이사의 경영진단 및 분석의견\"' in str(pages[i].find('head')):\n",
    "                body10.append(\n",
    "                    str(pages[i].find('head')).split('IV. 이사의 경영진단 및 분석의견\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            if 'IV. 이사의 경영진단 및 분석의견\"' not in str(pages[i].find('head')) and 'V. 이사의 경영진단 및 분석의견\"' in str(pages[i].find('head')):\n",
    "                body10.append(\n",
    "                    str(pages[i].find('head')).split('V. 이사의 경영진단 및 분석의견\",')[1].split('cnt++')[0].split('viewDoc(')[1].split(')')[0].split(', ')\n",
    "                )\n",
    "            if ('IV. 이사의 경영진단 및 분석의견\"' and 'V. 이사의 경영진단 및 분석의견\"') not in str(pages[i].find('head')):\n",
    "                pass\n",
    "\n",
    "        \n",
    "        bodies1 = [body1[i][0:-1] for i in range(len(body1))]\n",
    "        bodies2 = [body2[i][0:-1] for i in range(len(body2))]\n",
    "        bodies3 = [body3[i][0:-1] for i in range(len(body3))]\n",
    "        bodies4 = [body4[i][0:-1] for i in range(len(body4))]\n",
    "        bodies5 = [body5[i][0:-1] for i in range(len(body5))]\n",
    "        bodies6 = [body6[i][0:-1] for i in range(len(body6))]\n",
    "        bodies7 = [body7[i][0:-1] for i in range(len(body7))]\n",
    "        bodies8 = [body8[i][0:-1] for i in range(len(body8))]\n",
    "        bodies9 = [body9[i][0:-1] for i in range(len(body9))]\n",
    "        bodies10 = [body10[i][0:-1] for i in range(len(body10))]\n",
    "\n",
    "\n",
    "        # final url 만들기\n",
    "        urls_final1 = []\n",
    "        urls_final2 = []\n",
    "        urls_final3 = []\n",
    "        urls_final4 = []\n",
    "        urls_final5 = []\n",
    "        urls_final6 = []\n",
    "        urls_final7 = []\n",
    "        urls_final8 = []\n",
    "        urls_final9 = []\n",
    "        urls_final10 = []\n",
    "\n",
    "\n",
    "    # 사업보고서 부문별 텍스트 최종 링크 생성\n",
    "        for i in range(len(bodies1)):\n",
    "            urls_final1.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies1[i][0].strip(\"''\") + '&dcmNo=' + bodies1[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies1[i][2].strip(\"''\") + '&offset=' + bodies1[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies1[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies2)):\n",
    "            urls_final2.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies2[i][0].strip(\"''\") + '&dcmNo=' + bodies2[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies2[i][2].strip(\"''\") + '&offset=' + bodies2[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies2[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies3)):\n",
    "            urls_final3.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies3[i][0].strip(\"''\") + '&dcmNo=' + bodies3[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies3[i][2].strip(\"''\") + '&offset=' + bodies3[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies3[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies4)):\n",
    "            urls_final4.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies4[i][0].strip(\"''\") + '&dcmNo=' + bodies4[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies4[i][2].strip(\"''\") + '&offset=' + bodies4[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies4[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies5)):\n",
    "            urls_final5.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies5[i][0].strip(\"''\") + '&dcmNo=' + bodies5[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies5[i][2].strip(\"''\") + '&offset=' + bodies5[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies5[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies6)):\n",
    "            urls_final6.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies6[i][0].strip(\"''\") + '&dcmNo=' + bodies6[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies6[i][2].strip(\"''\") + '&offset=' + bodies6[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies6[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies7)):\n",
    "            urls_final7.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies7[i][0].strip(\"''\") + '&dcmNo=' + bodies7[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies7[i][2].strip(\"''\") + '&offset=' + bodies7[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies7[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies8)):\n",
    "            urls_final8.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies8[i][0].strip(\"''\") + '&dcmNo=' + bodies8[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies8[i][2].strip(\"''\") + '&offset=' + bodies8[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies8[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies9)):\n",
    "            urls_final9.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies9[i][0].strip(\"''\") + '&dcmNo=' + bodies9[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies9[i][2].strip(\"''\") + '&offset=' + bodies9[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies9[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        for i in range(len(bodies10)):\n",
    "            urls_final10.append(\n",
    "                'http://dart.fss.or.kr/report/viewer.do?rcpNo=' + bodies10[i][0].strip(\"''\") + '&dcmNo=' + bodies10[i][\n",
    "                    1].strip(\"''\") + '&eleId=' + bodies10[i][2].strip(\"''\") + '&offset=' + bodies10[i][3].strip(\n",
    "                    \"''\") + '&length=' + bodies10[i][4].strip(\"''\") + '&dtd=dart3.xsd')\n",
    "\n",
    "        # 최종 URL 링크로 들어가 텍스트 가져오기\n",
    "\n",
    "        body_final1 = []\n",
    "        body_final2 = []\n",
    "        body_final3 = []\n",
    "        body_final4 = []\n",
    "        body_final5 = []\n",
    "        body_final6 = []\n",
    "        body_final7 = []\n",
    "        body_final8 = []\n",
    "        body_final9 = []\n",
    "        body_final10 = []\n",
    "        \n",
    "        # 수집된 텍스트 저장할 위치 설정\n",
    "        data_folder_path = \"D:/2020/paper/data_folder_alltag_newversion/\"  + data['corp_nm'][0] + \"_Report\"\n",
    "        Path(data_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        content_list = ['content1', 'content2', 'content3', 'content4', 'content5', 'content6', 'content7', 'content8', 'content9', 'content10']\n",
    "\n",
    "        data_file_path1 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[0] + \".txt\"\n",
    "        data_file_path2 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[1] + \".txt\"\n",
    "        data_file_path3 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[2] + \".txt\"\n",
    "        data_file_path4 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[3] + \".txt\"\n",
    "        data_file_path5 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[4] + \".txt\"\n",
    "        data_file_path6 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[5] + \".txt\"\n",
    "        data_file_path7 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[6] + \".txt\"\n",
    "        data_file_path8 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[7] + \".txt\"\n",
    "        data_file_path9 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[8] + \".txt\"\n",
    "        data_file_path10 = data_folder_path + '/' + data['corp_nm'][0] + \"_\" + content_list[9] + \".txt\"\n",
    "\n",
    "        \n",
    "        # 항목1\n",
    "        for i in range(len(urls_final1)):\n",
    "            time.sleep(0.5)\n",
    "            body_final1.append([BeautifulSoup(urlopen(urls_final1[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final1 = []\n",
    "        for i in tqdm(range(len(body_final1))):\n",
    "            for j in range(len(body_final1[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final1.append([str(body_final1[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path1, \"w\", encoding='UTF-8') as test_output1:\n",
    "            text_output1 = test_output1.write(str(texts_final1))\n",
    "            \n",
    "            \n",
    "        # 항목2\n",
    "        for i in range(len(urls_final2)):\n",
    "            time.sleep(0.5)\n",
    "            body_final2.append([BeautifulSoup(urlopen(urls_final2[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final2 = []\n",
    "        for i in tqdm(range(len(body_final2))):\n",
    "            for j in range(len(body_final2[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final2.append([str(body_final2[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path2, \"w\", encoding='UTF-8') as test_output2:\n",
    "            text_output2 = test_output2.write(str(texts_final2))\n",
    "\n",
    "            \n",
    "        # 항목3\n",
    "        for i in range(len(urls_final3)):\n",
    "            time.sleep(0.5)\n",
    "            body_final3.append([BeautifulSoup(urlopen(urls_final3[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final3 = []\n",
    "        for i in tqdm(range(len(body_final3))):\n",
    "            for j in range(len(body_final3[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final3.append([str(body_final3[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path3, \"w\", encoding='UTF-8') as test_output3:\n",
    "            text_output3 = test_output3.write(str(texts_final3))\n",
    "\n",
    "        \n",
    "        # 항목4\n",
    "        for i in range(len(urls_final4)):\n",
    "            time.sleep(0.5)\n",
    "            body_final4.append([BeautifulSoup(urlopen(urls_final4[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final4 = []\n",
    "        for i in tqdm(range(len(body_final4))):\n",
    "            for j in range(len(body_final4[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final4.append([str(body_final4[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path4, \"w\", encoding='UTF-8') as test_output4:\n",
    "            text_output4 = test_output4.write(str(texts_final4))\n",
    "\n",
    "\n",
    "\n",
    "        # 항목5\n",
    "        for i in range(len(urls_final5)):\n",
    "            time.sleep(0.5)\n",
    "            body_final5.append([BeautifulSoup(urlopen(urls_final5[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final5 = []\n",
    "        for i in tqdm(range(len(body_final5))):\n",
    "            for j in range(len(body_final5[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final5.append([str(body_final5[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path5, \"w\", encoding='UTF-8') as test_output5:\n",
    "            text_output5 = test_output5.write(str(texts_final5))\n",
    "\n",
    "        \n",
    "\n",
    "        # 항목6\n",
    "        for i in range(len(urls_final6)):\n",
    "            time.sleep(0.5)\n",
    "            body_final6.append([BeautifulSoup(urlopen(urls_final6[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final6 = []\n",
    "        for i in tqdm(range(len(body_final6))):\n",
    "            for j in range(len(body_final6[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final6.append([str(body_final6[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path6, \"w\", encoding='UTF-8') as test_output6:\n",
    "            text_output6 = test_output6.write(str(texts_final6))\n",
    "\n",
    "        \n",
    "\n",
    "        # 항목7\n",
    "        for i in range(len(urls_final7)):\n",
    "            time.sleep(0.5)\n",
    "            body_final7.append([BeautifulSoup(urlopen(urls_final7[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final7 = []\n",
    "        for i in tqdm(range(len(body_final7))):\n",
    "            for j in range(len(body_final7[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final7.append([str(body_final7[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path7, \"w\", encoding='UTF-8') as test_output7:\n",
    "            text_output7 = test_output7.write(str(texts_final7))\n",
    "\n",
    "        \n",
    "\n",
    "        # 항목8\n",
    "        for i in range(len(urls_final8)):\n",
    "            time.sleep(0.5)\n",
    "            body_final8.append([BeautifulSoup(urlopen(urls_final8[i]).read(), 'html.parser')])\n",
    "\n",
    "        # 01 / 23 / 45 / 67 / 89 / 1011\n",
    "        year = datetime.date.today().year\n",
    "        texts_final8 = []\n",
    "        try:\n",
    "            for i in tqdm(range(0, (year - 2014)*2, 2)):\n",
    "                time.sleep(0.5)\n",
    "                texts_final8.append([str(body_final8[i][0].find_all('body')) + str(body_final8[i + 1][0].find_all('body'))])\n",
    "            for i in tqdm(range((year - 2014)*2, len(body_final8), 1)):\n",
    "                time.sleep(0.5)\n",
    "                texts_final8.append([str(body_final8[i][0].find_all('body'))])\n",
    "        except:\n",
    "            for i in tqdm(range(len(body_final8))):\n",
    "                for j in range(len(body_final8[i])):\n",
    "                    time.sleep(0.5)\n",
    "                    texts_final8.append([str(body_final8[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path8, \"w\", encoding='UTF-8') as test_output8:\n",
    "            text_output8 = test_output8.write(str(texts_final8))\n",
    "\n",
    "\n",
    "        # 항목9\n",
    "        for i in range(len(urls_final9)):\n",
    "            time.sleep(0.5)\n",
    "            body_final9.append([BeautifulSoup(urlopen(urls_final9[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final9 = []\n",
    "        for i in tqdm(range(len(body_final9))):\n",
    "            for j in range(len(body_final9[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final9.append([str(body_final9[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path9, \"w\", encoding='UTF-8') as test_output9:\n",
    "            text_output9 = test_output9.write(str(texts_final9))\n",
    "\n",
    "        \n",
    "        # 항목10\n",
    "        for i in range(len(urls_final10)):\n",
    "            time.sleep(0.5)\n",
    "            body_final10.append([BeautifulSoup(urlopen(urls_final10[i]).read(), 'html.parser')])\n",
    "\n",
    "        texts_final10 = []\n",
    "        for i in tqdm(range(len(body_final10))):\n",
    "            for j in range(len(body_final10[i])):\n",
    "                time.sleep(0.5)\n",
    "                texts_final10.append([str(body_final10[i][j].find_all('body'))])\n",
    "\n",
    "        with open(data_file_path10, \"w\", encoding='UTF-8') as test_output10:\n",
    "            text_output10 = test_output10.write(str(texts_final10))\n",
    "\n",
    "        end = '모든 KOSPI 상장기업 사업보고서 추출 완료'\n",
    "\n",
    "        return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRX 상장기업 리스트 코드를 그대로 사용하면, 사업보고서가 없는.. 리츠 같은 불필요한 종목들이 목록에 반영됨, \n",
    "# 따라서 fnguide를 기준으로 종목 필터링\n",
    "corp_list = pd.read_csv('D:/2020/paper/0.text_crawler/kospi_code_filter.csv', encoding='UTF-8')\n",
    "temp_name = []\n",
    "temp_code = []\n",
    "for i in range(len(corp_list['code_krx'])):\n",
    "    if list(corp_list['code_krx'])[i] in list(corp_list['code_fg']):\n",
    "        temp_name.append(corp_list['name'][i])\n",
    "        temp_code.append(corp_list['code_krx'][i])\n",
    "temp_name = pd.DataFrame(temp_name)\n",
    "temp_code = pd.DataFrame(temp_code)\n",
    "corp_filtered = pd.concat([temp_code, temp_name],1)\n",
    "corp_filtered.columns = ['code','name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사업보고서 크롤링 시작\n",
    "today = datetime.datetime.today()\n",
    "start_time = time.time()\n",
    "for i in range(len(corp_filtered)):\n",
    "    start_time_inner = time.time()\n",
    "    Year_Report_Crawler_v2(str(corp_filtered['code'][i]).zfill(6), today)\n",
    "    print(str(i) + ' 번째 기업 ' + corp_filtered['name'][i] + \" (기업코드 : A\" + str(corp_filtered['code'][i]).zfill(\n",
    "        6) + \") 의 10-Q ~ 10-K Report.. 10개 항목 전체기간 텍스트 데이터 크롤링에 소요된 시간..\" + str(\n",
    "        round((time.time() - start_time_inner) / 60, 2)) + \"분\")\n",
    "print(str(len(corp_filtered['code'])) + \" 개 기업, 사업보고서 텍스트 데이터 크롤링에 소요한 총 시간: \" + str(round(float(time.time() - start_time_inner) / 60, 2)) + \"분\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
